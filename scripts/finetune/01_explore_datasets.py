"""
scripts/finetune/01_explore_datasets.py
Phase 1: 3ê°œ ë°ì´í„°ì…‹ ë‹¤ìš´ë¡œë“œ ë° ê¸°ì´ˆ íƒìƒ‰.

ì‚¬ìš©ë²•:
    uv run python scripts/finetune/01_explore_datasets.py

ìƒˆ ë°ì´í„°ì…‹ ì¶”ê°€ ì‹œ:
    1. src/qcd/loaders.py ì— ë¡œë” í´ë˜ìŠ¤ ì¶”ê°€
    2. ì•„ë˜ DATASET_SOURCES ë”•ì…”ë„ˆë¦¬ì— í•­ëª© ì¶”ê°€
"""

import json
import urllib.request
from pathlib import Path

import kagglehub
from datasets import load_dataset
from kagglehub import KaggleDatasetAdapter

from qcd.loaders import LOADERS

# â”€â”€ ë°ì´í„°ì…‹ ë‹¤ìš´ë¡œë“œ ì†ŒìŠ¤ ì •ì˜ â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# ìƒˆ ë°ì´í„°ì…‹ ì¶”ê°€ ì‹œ ì—¬ê¸°ì—ë§Œ í•­ëª©ì„ ì¶”ê°€í•˜ë©´ ë©ë‹ˆë‹¤.
DATASET_SOURCES: dict[str, dict] = {
    "ourafla": {
        "type":    "huggingface",
        "hf_repo": "ourafla/Mental-Health_Text-Classification_Dataset",
        # feature_engineered.csvëŠ” ì»¬ëŸ¼ ìˆ˜ê°€ ë‹¬ë¼ ìŠ¤í‚¤ë§ˆ ì¶©ëŒ â†’ ì›í•˜ëŠ” íŒŒì¼ë§Œ ì§€ì •
        "hf_files": {
            "train": "mental_heath_unbanlanced.csv",
            "test":  "mental_health_combined_test.csv",
        },
        "raw_dir": Path("data/finetune_raw/ourafla"),
    },
    "depressionemo": {
        "type":       "github_json",
        "base_url":   "https://raw.githubusercontent.com/abuBakarSiddiqurRahman/DepressionEmo/main",
        "files":      ["train.json", "val.json", "test.json"],
        "raw_dir":    Path("data/finetune_raw/depressionemo"),
    },
    "cssrs": {
        "type":       "kaggle",
        "kaggle_id":  "av9ash/labelled-reddit-suicidewatch-posts-cssr-s",
        "raw_dir":    Path("data/finetune_raw/cssrs"),
    },
}


# â”€â”€ ë‹¤ìš´ë¡œë“œ í•¨ìˆ˜ â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

def _download_huggingface(name: str, cfg: dict) -> None:
    raw_dir: Path = cfg["raw_dir"]
    if raw_dir.exists() and any(raw_dir.glob("*.csv")):
        print(f"[{name}] ì´ë¯¸ ë‹¤ìš´ë¡œë“œë¨, ê±´ë„ˆëœ€")
        return

    print(f"[{name}] HuggingFace ë‹¤ìš´ë¡œë“œ ì¤‘: {cfg['hf_repo']}")
    raw_dir.mkdir(parents=True, exist_ok=True)
    if "hf_files" in cfg:
        # íŒŒì¼ë³„ ìŠ¤í‚¤ë§ˆê°€ ë‹¬ë¼ load_dataset()ì´ ì‹¤íŒ¨í•˜ëŠ” ê²½ìš° ê°œë³„ íŒŒì¼ ì§€ì •
        data_files = {
            split: f"hf://datasets/{cfg['hf_repo']}/{fname}"
            for split, fname in cfg["hf_files"].items()
        }
        ds = load_dataset("csv", data_files=data_files)
    else:
        ds = load_dataset(cfg["hf_repo"])
    for split_name, split_ds in ds.items():
        out = raw_dir / f"{split_name}.csv"
        split_ds.to_csv(str(out))
        print(f"  {split_name}: {len(split_ds):,}í–‰ â†’ {out}")


def _download_github_json(name: str, cfg: dict) -> None:
    raw_dir: Path = cfg["raw_dir"]
    if raw_dir.exists() and any(raw_dir.glob("*.json")):
        print(f"[{name}] ì´ë¯¸ ë‹¤ìš´ë¡œë“œë¨, ê±´ë„ˆëœ€")
        return

    print(f"[{name}] GitHub ë‹¤ìš´ë¡œë“œ ì¤‘: {cfg['base_url']}")
    raw_dir.mkdir(parents=True, exist_ok=True)
    for fname in cfg["files"]:
        url = f"{cfg['base_url']}/{fname}"
        dest = raw_dir / fname
        urllib.request.urlretrieve(url, str(dest))
        data = json.load(open(dest, encoding="utf-8"))
        print(f"  {fname}: {len(data):,}í–‰ â†’ {dest}")


def _download_kaggle(name: str, cfg: dict) -> None:
    raw_dir: Path = cfg["raw_dir"]
    if raw_dir.exists() and any(raw_dir.glob("*.csv")):
        print(f"[{name}] ì´ë¯¸ ë‹¤ìš´ë¡œë“œë¨, ê±´ë„ˆëœ€")
        return

    print(f"[{name}] Kaggle ë‹¤ìš´ë¡œë“œ ì¤‘: {cfg['kaggle_id']}")
    raw_dir.mkdir(parents=True, exist_ok=True)
    df = kagglehub.load_dataset(
        KaggleDatasetAdapter.PANDAS,
        cfg["kaggle_id"],
        "",
    )
    out = raw_dir / "data.csv"
    df.to_csv(out, index=False, encoding="utf-8")
    print(f"  ì „ì²´: {len(df):,}í–‰ â†’ {out}")


_DOWNLOADERS = {
    "huggingface": _download_huggingface,
    "github_json": _download_github_json,
    "kaggle":      _download_kaggle,
}


def download_all() -> None:
    """DATASET_SOURCESì— ì •ì˜ëœ ëª¨ë“  ë°ì´í„°ì…‹ì„ ë‹¤ìš´ë¡œë“œ."""
    for name, cfg in DATASET_SOURCES.items():
        _DOWNLOADERS[cfg["type"]](name, cfg)
    print()


# â”€â”€ íƒìƒ‰ í•¨ìˆ˜ â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

def _collect_files(raw_dir: Path, source_type: str) -> list[Path]:
    """ë°ì´í„°ì…‹ í´ë”ì—ì„œ ë¡œë”ê°€ ì½ì„ ìˆ˜ ìˆëŠ” íŒŒì¼ ëª©ë¡ ë°˜í™˜."""
    ext = "*.json" if source_type == "github_json" else "*.csv"
    return sorted(raw_dir.glob(ext))


def _print_stats(name: str, rows: list[dict], loader) -> None:
    """ë¡œë”ì—ì„œ ë°˜í™˜ëœ ë ˆì½”ë“œ ë¦¬ìŠ¤íŠ¸ì˜ í†µê³„ë¥¼ ì¶œë ¥ (raw í¬ë§· ê¸°ì¤€)."""
    if not rows:
        print("  âš ï¸ ë¡œë“œëœ ë°ì´í„° ì—†ìŒ")
        return

    print(f"  ì´ {len(rows):,}í–‰ | í‰ê·  í…ìŠ¤íŠ¸ {sum(len(r['text']) for r in rows) / len(rows):.0f}ì")

    # ë°ì´í„°ì…‹ë³„ ì›ë³¸ ë ˆì´ë¸”/ê°ì • ë¶„í¬ ì¶œë ¥
    source = rows[0].get("source", name)

    if source == "ourafla":
        dist: dict[str, int] = {}
        for r in rows:
            k = r.get("original_label", "?")
            dist[k] = dist.get(k, 0) + 1
        print(f"  ì›ë³¸ ë ˆì´ë¸” ë¶„í¬: {dict(sorted(dist.items()))}")

    elif source == "depressionemo":
        dist = {}
        for r in rows:
            for emo in r.get("emotions", []):
                dist[emo] = dist.get(emo, 0) + 1
        sorted_dist = dict(sorted(dist.items(), key=lambda x: -x[1]))
        print(f"  ê°ì • ë¶„í¬ (ë©€í‹°ë ˆì´ë¸”): {sorted_dist}")

    elif source == "cssrs":
        dist = {}
        for r in rows:
            k = r.get("cssrs_level", "?")
            dist[k] = dist.get(k, 0) + 1
        print(f"  C-SSRS ë ˆë²¨ ë¶„í¬: {dict(sorted(dist.items()))}")

    # ìƒ˜í”Œ 30ê°œ â€” loader.get_label()ë¡œ ë¼ë²¨ë³„ ê· ë“± ì¶”ì¶œ
    SAMPLE_TOTAL = 30

    groups: dict[str, list[dict]] = {}
    for r in rows:
        k = loader.get_label(r)
        groups.setdefault(k, []).append(r)

    n_per = max(1, SAMPLE_TOTAL // len(groups))
    samples = [r for grp in groups.values() for r in grp[:n_per]]

    print(f"  ìƒ˜í”Œ (ë¼ë²¨ë³„ ìµœëŒ€ {n_per}ê°œ):")
    for r in samples:
        preview = r["text"][:200].replace("\n", " ")
        print(f"    [{loader.get_label(r)}] {preview}")


def explore_all(targets: list[str] | None = None) -> None:
    """LOADERSì— ë“±ë¡ëœ íŒŒì¸íŠœë‹ ë¡œë”ë¡œ ê° ë°ì´í„°ì…‹ íƒìƒ‰.

    targetsê°€ Noneì´ë©´ DATASET_SOURCESì˜ ëª¨ë“  í‚¤ë¥¼ íƒìƒ‰.
    """
    names = targets if targets is not None else list(DATASET_SOURCES.keys())
    sep = "=" * 64

    print(sep)
    print("Phase 1 â€” ë°ì´í„°ì…‹ ê¸°ì´ˆ íƒìƒ‰")
    print(sep)

    for name in names:
        loader_cls = LOADERS.get(name)
        src_cfg = DATASET_SOURCES.get(name)
        if loader_cls is None or src_cfg is None:
            continue

        loader = loader_cls()
        files = _collect_files(src_cfg["raw_dir"], src_cfg["type"])

        print(f"\nğŸ“ {name} ({len(files)}ê°œ íŒŒì¼)")

        all_rows: list[dict] = []
        for fpath in files:
            rows = loader.load(fpath)
            all_rows.extend(rows)
            print(f"  â”” {fpath.name}: {len(rows):,}í–‰")

        _print_stats(name, all_rows, loader)


if __name__ == "__main__":
    import sys
    # ì¸ì ì—†ìœ¼ë©´ ì „ì²´ ì‹¤í–‰, ìˆìœ¼ë©´ í•´ë‹¹ ë°ì´í„°ì…‹ë§Œ
    # ì˜ˆ) uv run python scripts/finetune/01_explore_datasets.py ourafla cssrs
    targets = sys.argv[1:] or list(DATASET_SOURCES.keys())
    unknown = [t for t in targets if t not in DATASET_SOURCES]
    if unknown:
        print(f"[ì˜¤ë¥˜] ì•Œ ìˆ˜ ì—†ëŠ” ë°ì´í„°ì…‹: {unknown}")
        print(f"  ì‚¬ìš© ê°€ëŠ¥: {list(DATASET_SOURCES.keys())}")
        sys.exit(1)

    for name in targets:
        cfg = DATASET_SOURCES[name]
        _DOWNLOADERS[cfg["type"]](name, cfg)
    print()
    explore_all(targets)
