"""
scripts/finetune/01_explore_datasets.py
Phase 1: 3ê°œ ë°ì´í„°ì…‹ ë‹¤ìš´ë¡œë“œ ë° ê¸°ì´ˆ íƒìƒ‰.

ì‚¬ìš©ë²•:
    uv run python scripts/finetune/01_explore_datasets.py

ìƒˆ ë°ì´í„°ì…‹ ì¶”ê°€ ì‹œ:
    1. src/qcd/loaders.py ì— ë¡œë” í´ë˜ìŠ¤ ì¶”ê°€
    2. ì•„ë˜ DATASET_SOURCES ë”•ì…”ë„ˆë¦¬ì— í•­ëª© ì¶”ê°€
"""

import json
import urllib.request
from pathlib import Path

import kagglehub
from datasets import load_dataset
from kagglehub import KaggleDatasetAdapter

from qcd.loaders import LOADERS

# â”€â”€ ë°ì´í„°ì…‹ ë‹¤ìš´ë¡œë“œ ì†ŒìŠ¤ ì •ì˜ â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# ìƒˆ ë°ì´í„°ì…‹ ì¶”ê°€ ì‹œ ì—¬ê¸°ì—ë§Œ í•­ëª©ì„ ì¶”ê°€í•˜ë©´ ë©ë‹ˆë‹¤.
DATASET_SOURCES: dict[str, dict] = {
    "ourafla": {
        "type":    "huggingface",
        "hf_repo": "ourafla/Mental-Health_Text-Classification_Dataset",
        "raw_dir": Path("data/finetune_raw/ourafla"),
    },
    "depressionemo": {
        "type":       "github_json",
        "base_url":   "https://raw.githubusercontent.com/abuBakarSiddiqurRahman/DepressionEmo/main",
        "files":      ["train.json", "val.json", "test.json"],
        "raw_dir":    Path("data/finetune_raw/depressionemo"),
    },
    "cssrs": {
        "type":       "kaggle",
        "kaggle_id":  "av9ash/labelled-reddit-suicidewatch-posts-cssr-s",
        "raw_dir":    Path("data/finetune_raw/cssrs"),
    },
}


# â”€â”€ ë‹¤ìš´ë¡œë“œ í•¨ìˆ˜ â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

def _download_huggingface(name: str, cfg: dict) -> None:
    raw_dir: Path = cfg["raw_dir"]
    if raw_dir.exists() and any(raw_dir.glob("*.csv")):
        print(f"[{name}] ì´ë¯¸ ë‹¤ìš´ë¡œë“œë¨, ê±´ë„ˆëœ€")
        return

    print(f"[{name}] HuggingFace ë‹¤ìš´ë¡œë“œ ì¤‘: {cfg['hf_repo']}")
    raw_dir.mkdir(parents=True, exist_ok=True)
    ds = load_dataset(cfg["hf_repo"])
    for split_name, split_ds in ds.items():
        out = raw_dir / f"{split_name}.csv"
        split_ds.to_csv(str(out))
        print(f"  {split_name}: {len(split_ds):,}í–‰ â†’ {out}")


def _download_github_json(name: str, cfg: dict) -> None:
    raw_dir: Path = cfg["raw_dir"]
    if raw_dir.exists() and any(raw_dir.glob("*.json")):
        print(f"[{name}] ì´ë¯¸ ë‹¤ìš´ë¡œë“œë¨, ê±´ë„ˆëœ€")
        return

    print(f"[{name}] GitHub ë‹¤ìš´ë¡œë“œ ì¤‘: {cfg['base_url']}")
    raw_dir.mkdir(parents=True, exist_ok=True)
    for fname in cfg["files"]:
        url = f"{cfg['base_url']}/{fname}"
        dest = raw_dir / fname
        urllib.request.urlretrieve(url, str(dest))
        data = json.load(open(dest, encoding="utf-8"))
        print(f"  {fname}: {len(data):,}í–‰ â†’ {dest}")


def _download_kaggle(name: str, cfg: dict) -> None:
    raw_dir: Path = cfg["raw_dir"]
    if raw_dir.exists() and any(raw_dir.glob("*.csv")):
        print(f"[{name}] ì´ë¯¸ ë‹¤ìš´ë¡œë“œë¨, ê±´ë„ˆëœ€")
        return

    print(f"[{name}] Kaggle ë‹¤ìš´ë¡œë“œ ì¤‘: {cfg['kaggle_id']}")
    raw_dir.mkdir(parents=True, exist_ok=True)
    df = kagglehub.load_dataset(
        KaggleDatasetAdapter.PANDAS,
        cfg["kaggle_id"],
        "",
    )
    out = raw_dir / "data.csv"
    df.to_csv(out, index=False, encoding="utf-8")
    print(f"  ì „ì²´: {len(df):,}í–‰ â†’ {out}")


_DOWNLOADERS = {
    "huggingface": _download_huggingface,
    "github_json": _download_github_json,
    "kaggle":      _download_kaggle,
}


def download_all() -> None:
    """DATASET_SOURCESì— ì •ì˜ëœ ëª¨ë“  ë°ì´í„°ì…‹ì„ ë‹¤ìš´ë¡œë“œ."""
    for name, cfg in DATASET_SOURCES.items():
        _DOWNLOADERS[cfg["type"]](name, cfg)
    print()


# â”€â”€ íƒìƒ‰ í•¨ìˆ˜ â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

def _collect_files(raw_dir: Path, source_type: str) -> list[Path]:
    """ë°ì´í„°ì…‹ í´ë”ì—ì„œ ë¡œë”ê°€ ì½ì„ ìˆ˜ ìˆëŠ” íŒŒì¼ ëª©ë¡ ë°˜í™˜."""
    ext = "*.json" if source_type == "github_json" else "*.csv"
    return sorted(raw_dir.glob(ext))


def _print_stats(name: str, rows: list[dict]) -> None:
    """ë¡œë”ì—ì„œ ë°˜í™˜ëœ ë ˆì½”ë“œ ë¦¬ìŠ¤íŠ¸ì˜ í†µê³„ë¥¼ ì¶œë ¥ (raw í¬ë§· ê¸°ì¤€)."""
    if not rows:
        print("  âš ï¸ ë¡œë“œëœ ë°ì´í„° ì—†ìŒ")
        return

    print(f"  ì´ {len(rows):,}í–‰ | í‰ê·  í…ìŠ¤íŠ¸ {sum(len(r['text']) for r in rows) / len(rows):.0f}ì")

    # ë°ì´í„°ì…‹ë³„ ì›ë³¸ ë ˆì´ë¸”/ê°ì • ë¶„í¬ ì¶œë ¥
    source = rows[0].get("source", name)

    if source == "ourafla":
        dist: dict[str, int] = {}
        for r in rows:
            k = r.get("original_label", "?")
            dist[k] = dist.get(k, 0) + 1
        print(f"  ì›ë³¸ ë ˆì´ë¸” ë¶„í¬: {dict(sorted(dist.items()))}")

    elif source == "depressionemo":
        dist = {}
        for r in rows:
            for emo in r.get("emotions", []):
                dist[emo] = dist.get(emo, 0) + 1
        sorted_dist = dict(sorted(dist.items(), key=lambda x: -x[1]))
        print(f"  ê°ì • ë¶„í¬ (ë©€í‹°ë ˆì´ë¸”): {sorted_dist}")

    elif source == "cssrs":
        dist = {}
        for r in rows:
            k = r.get("cssrs_level", "?")
            dist[k] = dist.get(k, 0) + 1
        print(f"  C-SSRS ë ˆë²¨ ë¶„í¬: {dict(sorted(dist.items()))}")

    # ìƒ˜í”Œ 5ê°œ
    print("  ìƒ˜í”Œ:")
    for r in rows[:5]:
        preview = r["text"][:100].replace("\n", " ")
        if source == "ourafla":
            tag = r.get("original_label", "?")
        elif source == "depressionemo":
            tag = str(r.get("emotions", []))
        else:
            tag = f"level={r.get('cssrs_level', '?')}"
        print(f"    [{tag}] {preview}")


def explore_all() -> None:
    """LOADERSì— ë“±ë¡ëœ íŒŒì¸íŠœë‹ ë¡œë”ë¡œ ê° ë°ì´í„°ì…‹ íƒìƒ‰."""
    FINETUNE_LOADERS = ("ourafla", "depressionemo", "cssrs")
    sep = "=" * 64

    print(sep)
    print("Phase 1 â€” ë°ì´í„°ì…‹ ê¸°ì´ˆ íƒìƒ‰")
    print(sep)

    for name in FINETUNE_LOADERS:
        loader_cls = LOADERS.get(name)
        src_cfg = DATASET_SOURCES.get(name)
        if loader_cls is None or src_cfg is None:
            continue

        loader = loader_cls()
        files = _collect_files(src_cfg["raw_dir"], src_cfg["type"])

        print(f"\nğŸ“ {name} ({len(files)}ê°œ íŒŒì¼)")

        all_rows: list[dict] = []
        for fpath in files:
            rows = loader.load(fpath)
            all_rows.extend(rows)
            print(f"  â”” {fpath.name}: {len(rows):,}í–‰")

        _print_stats(name, all_rows)


if __name__ == "__main__":
    download_all()
    explore_all()
